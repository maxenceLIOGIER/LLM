"""
Page d'un chatbot m√©dical
ATTENTION : le llm ne parle pas fran√ßais.
Il faut mieux √©laborer son prompt
"""

import os
import time
import streamlit as st
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain import hub
from langgraph.graph import StateGraph, START
from langchain_core.documents import Document
from typing_extensions import List, TypedDict

from views.dashboard import track_metrics

load_dotenv()

# V√©rifier que la cl√© API
hf_api_key = os.getenv("HF_API_KEY")

if not hf_api_key:
    st.error("‚ö†Ô∏è Erreur API.")
    st.stop()


# Fonction pour initialiser le mod√®le et les ressources (ne s'ex√©cute qu'une seule fois)
def initialize_resources():
    if "llm" not in st.session_state:
        st.session_state.llm = HuggingFaceEndpoint(
            repo_id="mistralai/Mistral-7B-Instruct-v0.2",
            huggingfacehub_api_token=hf_api_key,
            task="text-generation",
        )

    if "embeddings" not in st.session_state:
        st.session_state.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )

    if "docs_embeddings" not in st.session_state:
        persist_directory = "./embed_s1000_o100"
        st.session_state.docs_embeddings = Chroma(
            collection_name="statpearls_articles",
            embedding_function=st.session_state.embeddings,
            persist_directory=persist_directory,
        )

    if "graph" not in st.session_state:
        prompt = hub.pull("rlm/rag-prompt")

        class State(TypedDict):
            question: str
            context: List[Document]
            answer: str

        def retrieve(state: State):
            retrieved_docs = st.session_state.docs_embeddings.similarity_search(
                state["question"], k=5
            )
            return {"context": retrieved_docs}

        def generate(state: State):
            docs_content = "\n\n".join(doc.page_content for doc in state["context"])
            messages = prompt.invoke(
                {"question": state["question"], "context": docs_content}
            )
            response = st.session_state.llm.invoke(messages)
            return {"answer": response}

        # Construire et stocker le graphe
        rag_graph = StateGraph(State)
        rag_graph.add_node("retrieve", retrieve)
        rag_graph.add_node("generate", generate)
        rag_graph.add_edge(START, "retrieve")
        rag_graph.add_edge("retrieve", "generate")
        st.session_state.graph = rag_graph.compile()


# Initialiser une seule fois
initialize_resources()


def get_response(question: str):
    """G√®re la requ√™te et enregistre les m√©triques"""
    start_time = time.time()

    state = {"question": question}
    result = st.session_state.graph.invoke(state)
    response = result["answer"]

    # Estimation
    token_count = len(response.split())
    latency = (time.time() - start_time) * 1000  # en ms

    # Mettre √† jour les m√©triques
    track_metrics(latency, token_count)

    return response


def chatbot_page():
    """Interface RAG sous forme de chat"""
    st.title("üí¨ Chat Medical")

    # Initialiser l'historique de conversation
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Affichage des messages pr√©c√©dents
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Zone de saisie pour l'utilisateur
    question = st.chat_input("Posez votre question...")

    if question:
        # Ajouter la question √† l'historique
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        # Obtenir la r√©ponse sans r√©initialiser le mod√®le
        response = get_response(question)

        # Ajouter la r√©ponse de l'IA √† l'historie
        st.session_state.messages.append({"role": "assistant", "content": response})
        with st.chat_message("assistant"):
            st.markdown(response)
