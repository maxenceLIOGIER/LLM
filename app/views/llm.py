import os
import sys
import datetime
import streamlit as st
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.prompts import PromptTemplate

from src.speech_to_text import WhisperLiveTranscription

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

load_dotenv()
HF_API_KEY = os.getenv("HF_API_KEY")

# Instanciation du transcripteur
transcriber = WhisperLiveTranscription(
    model_id="openai/whisper-small", language="french"
)
# Ne marche pas tr√®s bien pour les transcriptions √† la vol√©e


def llm_page():
    """
    Page de requ√™te du LLM en passant par un speech-to-text.
    Pour l'instant le LLM est appel√© √† la fin de l'enregistrement.
    A terme, il faudra appeler le LLM pendant l'enregistrement pour une discussion en temps r√©el.
    (toutes les x secondes ou tous les x caract√®res)

    ATTENTION !! dans la transcription, la derni√®re phrase est r√©p√©t√©e 3 fois ce qui fausse le mod√®le.
    """

    st.title("Requ√™te du mod√®le")
    st.subheader("Interrogez le LLM via votre voix ou texte")

    # Initialisation de l'√©tat de session
    if "recording" not in st.session_state:
        st.session_state.recording = False
    if "transcription" not in st.session_state:
        st.session_state.transcription = ""
    if "text_query" not in st.session_state:
        st.session_state.text_query = ""
    if "file" not in st.session_state:
        st.session_state.file = ""

    # Contr√¥les d'enregistrement
    col1, col2 = st.columns(2)

    if col1.button("üé§ D√©marrer l'enregistrement"):
        st.session_state.recording = True

        # Cr√©ation du fichier o√π les transcriptions seront stock√©es
        # timestamp YYYYMMDD_HHMM :
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
        # Cr√©er le fichier texte vide avec son timestamp
        st.session_state.file = f"transcription_{timestamp}.txt"
        with open(st.session_state.file, "a"):
            pass
            # on ira ensuite le remplir via speech_to_text.py

        transcriber.start_recording()
        st.info("Enregistrement en cours...")

    if col2.button("‚èπÔ∏è Arr√™ter l'enregistrement"):
        if st.session_state.recording:
            st.session_state.recording = False
            transcriber.stop_recording()
            st.success("Enregistrement termin√©")

        # Affichage de la transcription
        with open(st.session_state.file, "r", encoding="utf-8") as f:
            transcription = f.read()
        st.session_state.transcription = transcription

        if st.session_state.transcription:
            st.session_state.text_query = st.session_state.transcription
            # st.text_area("Transcription :", st.session_state.text_query, height=200)
        else:
            st.warning("Aucune transcription trouv√©e")

    # R√©sultat du LLM
    if st.button("Soumettre"):

        # template = "You are an artificial intelligence assistant, answer the question. {question}"
        template = "Un LLM con√ßu pour assister les agents des urgences en analysant leurs appels. \
            Ton : empathique, calme, direct, professionnel. \
            Objectif : extraire les informations critiques. (diagnostic, localisation, √©tat des personnes, danger)\
            Toujours rester pr√©cis et rapide. \
            Voici la discussion : {text_query}"

        prompt = PromptTemplate(template=template, input_variables=["text_query"])

        llm = HuggingFaceEndpoint(
            repo_id="tiiuae/falcon-7b-instruct", huggingfacehub_api_token=HF_API_KEY
        )
        llm_chain = prompt | llm

        st.write(
            "R√©ponse du LLM :",
            llm_chain.invoke({"text_query": st.session_state.text_query}),
        )
