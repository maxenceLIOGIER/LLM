import sqlite3
from datetime import datetime, timedelta
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import sendgrid
from sendgrid.helpers.mail import Mail, Email, To, Content
import os
from dotenv import load_dotenv

# Charger les variabels d'environnement
load_dotenv()
SENDGRID_API_KEY = os.getenv("SENDGRID_API_KEY")
FROM_EMAIL = os.getenv("FROM_EMAIL")
RECIPIENT_EMAIL = os.getenv("RECIPIENT_EMAIL")

# Chemin vers la DB
db_path = "sqlite:///../../database/db_logs.db"


class SecurityReport:
    def __init__(
        self,
        db_path=db_path,
        sendgrid_api_key=SENDGRID_API_KEY,
        from_email=FROM_EMAIL,
        recipient_email=RECIPIENT_EMAIL,
    ):
        self.DB_PATH = db_path
        self.sendgrid_api_key = sendgrid_api_key
        self.from_email = from_email
        self.recipient_email = recipient_email

    def query_logs(self, day:str = None) -> pd.DataFrame:
        """
        R√©cup√®re les logs de la journ√©e sous forme de DataFrame.

        Cette fonction effectue les op√©rations suivantes :
        1. D√©termine les horaires de d√©but et de fin de la journ√©e actuelle.
        2. √âtablit une connexion √† la base de donn√©es SQLite.
        3. Ex√©cute une requ√™te SQL pour r√©cup√©rer les logs du jour en effectuant des jointures 
        avec les tables `prompt`, `status` et `origin` afin d'obtenir des informations 
        d√©taill√©es sur chaque log.
        4. Retourne les donn√©es sous forme d'un DataFrame pandas.

        Returns:
            pd.DataFrame: Un DataFrame contenant les logs de la journ√©e avec les colonnes suivantes :
                - timestamp: Horodatage du log.
                - prompt: Texte de la requ√™te.
                - response: R√©ponse associ√©e.
                - status: Statut du log.
                - origin: Adresse IP de l'utilisateur.
        """

        # Connection √† la BDD et requete
        conn = sqlite3.connect(self.DB_PATH)
        query = """
            SELECT 
                log.timestamp AS timestamp,
                prompt.prompt AS prompt,
                prompt.response AS response,
                status.status AS status,
                origin.response AS origin
            FROM log
            LEFT JOIN prompt ON log.id_prompt = prompt.id_prompt
            LEFT JOIN status ON log.id_status = status.id_status
            LEFT JOIN origin ON log.id_origin = origin.id_origin
        """
        # Initialisation des param√®tres
        params = ()

        # Initialisation des horaires si date pr√©sente 
        if day:
            start_of_day = datetime.combine(day, datetime.min.time())
            end_of_day = datetime.combine(day, datetime.max.time())
            query += " WHERE log.timestamp BETWEEN ? AND ?"
            params = (start_of_day, end_of_day)

        # Logs r√©cup√©r√©s au format DataFrame
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()

        return df

    def _create_pipeline(self):
        """
        Cr√©e un pipeline de pr√©traitement des donn√©es pour le clustering.

        Cette fonction met en place un pipeline de transformation des donn√©es, qui comprend :
        1. Un pipeline sp√©cifique pour les donn√©es textuelles, appliquant une vectorisation TF-IDF
        avec un nombre de caract√©ristiques limit√© √† 50.
        2. Un pipeline pour les donn√©es cat√©gorielles, appliquant un encodage One-Hot tout en
        ignorant les valeurs inconnues lors de la transformation.
        3. Une combinaison de ces transformations √† l'aide d'un `ColumnTransformer` pour appliquer
        les transformations appropri√©es aux bonnes colonnes du dataset.
        4. Un pipeline principal qui applique ces transformations et normalise les donn√©es avec 
        `StandardScaler` (sans soustraction de la moyenne, car TF-IDF produit des matrices creuses).

        Returns:
            sklearn.pipeline.Pipeline: Un pipeline scikit-learn qui pr√©pare les donn√©es 
            avant leur utilisation en Machine Learning.
        """

        # Colonnes cat√©gorielles et textuelles
        categorical_features = ["status"]
        text_features = ["timestamp", "prompt", "response", "origin"]

        # Pipeline pour les donn√©es textuelles
        text_pipelines = {
            feature: Pipeline([("tfidf", TfidfVectorizer(max_features=50, stop_words=None, analyzer="word"))])
            for feature in text_features
        }

        # Pipeline pour les donn√©es catg√©orielles
        cat_pipeline = Pipeline(
            [("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))]
        )

        # Combinaison des m√©thodes
        preprocessor = ColumnTransformer(
            transformers=[
                ("text_" + feature, text_pipelines[feature], feature) 
                for feature in text_features
            ] + [("cat", cat_pipeline, categorical_features)],
            remainder="drop"
        )

        # Pipeline principale
        pipeline = Pipeline(
            [
                ("preprocessor", preprocessor),
                ("scaler", StandardScaler(with_mean=False)),
            ]
        )

        return pipeline

    def clustering_log(self, max_clusters:int=10) -> int:
        """
        Effectue un clustering sur les logs journaliers et d√©termine le nombre optimal de clusters.

        Cette fonction r√©alise les √©tapes suivantes :
        1. **Initialisation** :
        - D√©finit les variables pour suivre le meilleur score Silhouette et le nombre optimal de clusters.
        2. **R√©cup√©ration des logs journaliers** :
        - Charge les logs du jour via `query_daily_logs()`.
        3. **Pr√©traitement des donn√©es** :
        - Applique le pipeline de transformation `_create_pipeline()` pour pr√©parer les logs.
        4. **Clustering avec K-Means** :
        - Teste diff√©rentes valeurs de `n_clusters` (de 2 √† `max_clusters`).
        - Entra√Æne un mod√®le K-Means et calcule le **score de Silhouette** pour mesurer la qualit√© du clustering.
        - Identifie la valeur de `n_clusters` offrant le meilleur score.
        5. **Affichage des r√©sultats** :
        - Affiche les scores pour chaque nombre de clusters test√©.
        - Retourne le nombre optimal de clusters.

        Args:
            max_clusters (int, optional): Nombre maximal de clusters √† tester. Par d√©faut, 10.

        Returns:
            int: Le nombre optimal de clusters bas√© sur le meilleur score Silhouette.
        """
            
        # Initialisation des param√®tres
        best_score = -1  # Score Silhouette le plus √©lev√© trouv√©
        best_n_clusters = 2  # Nombre optimal de clusters

        # R√©cup√®re les logs journaliers
        logs = self.query_logs()

        # Pr√©traitement des logs
        preprocessor = self._create_pipeline()
        logs = preprocessor.fit_transform(logs)

        # Teste plusieurs nombres de clusters pour identifier le meilleur
        for n_clusters in range(2, max_clusters + 1):
            self.model = KMeans(n_clusters=n_clusters, random_state=0)
            self.model.fit(logs)
            
            # Calcul du score de Silhouette
            score = silhouette_score(logs, self.model.labels_)
            print(f"Nombre de clusters : {n_clusters}, Silhouette Score : {score:.4f}")

            # Mise √† jour du meilleur score et du meilleur nombre de clusters
            if score > best_score:
                best_score = score
                best_n_clusters = n_clusters

        # Affichage du meilleur nombre de clusters
        print(
            f"\nMeilleur nombre de clusters : {best_n_clusters}, Silhouette Score : {best_score:.4f}"
        )

        return best_n_clusters

    def generate_report(self, logs:pd.DataFrame) -> str:
        """
        G√©n√®re un rapport HTML sur les logs journaliers, incluant des statistiques et des r√©sultats de clustering.

        Cette fonction effectue les √©tapes suivantes :
        1. **Calcul des statistiques** :
        - R√©cup√®re la date actuelle.
        - Calcule le nombre total de logs.
        - Effectue un comptage des occurrences de chaque statut dans les logs.
        - Ex√©cute un clustering sur les logs pour d√©terminer le nombre de comportements diff√©rents.
        2. **Construction du rapport HTML** :
        - Cr√©e une page HTML contenant les statistiques sous forme de texte et de liste.
        - Ajoute un titre, les informations de r√©partition des statuts, et le nombre de clusters d√©tect√©s.
        - Applique un style simple pour rendre le rapport lisible et structur√©.
        3. **Retourne le rapport sous forme de cha√Æne HTML** :
        - Le rapport est sous forme de code HTML pr√™t √† √™tre envoy√© ou affich√©.

        Args:
            logs (pd.DataFrame): Un DataFrame contenant les logs √† analyser, avec au moins une colonne `status`.

        Returns:
            str: Un rapport HTML sous forme de cha√Æne de caract√®res.
        """
        
        # R√©cup√©ration de la date actuelle sous format dd/mm/yyyy
        date_str = datetime.now().strftime("%d/%m/%Y")
        
        # Nombre total de logs
        total_logs = len(logs)
        
        # Comptage des occurrences de chaque statut
        status_counts = logs["status"].value_counts().to_dict()
        
        # Ex√©cution du clustering pour obtenir le nombre de comportements diff√©rents d√©tect√©s
        n_clusters = self.clustering_log()

        # Cr√©ation d'une liste HTML des statuts et de leurs occurrences
        status_html = "".join(
            f"<li><strong>{status}:</strong> {count}</li>"
            for status, count in status_counts.items()
        )

        # Construction du rapport HTML
        report = f"""
        <html>
        <head>
            <style>
                body {{
                    font-family: Arial, sans-serif;
                    color: #333;
                    line-height: 1.6;
                }}
                .container {{
                    max-width: 600px;
                    margin: 20px auto;
                    padding: 20px;
                    border: 1px solid #ddd;
                    border-radius: 8px;
                    background-color: #f9f9f9;
                }}
                h2 {{
                    background-color: #007BFF;
                    color: white;
                    padding: 10px;
                    border-radius: 5px;
                    text-align: center;
                }}
                ul {{
                    list-style-type: none;
                    padding: 0;
                }}
                li {{
                    padding: 5px 0;
                }}
                .footer {{
                    margin-top: 20px;
                    font-size: 12px;
                    text-align: center;
                    color: #777;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h2>üìã Rapport de S√©curit√© - {date_str}</h2>
                <p><u><strong>Nombre total de logs :</strong></u> {total_logs}</p>
                <p><u><strong>R√©partition des statuts :</strong></u></p>
                <ul>
                    {status_html}
                </ul>
                <p><u><strong>üîç Nombre de comportements diff√©rents d√©tect√©s :</strong></u> {n_clusters}</p>
                <div class="footer">
                    Rapport g√©n√©r√© automatiquement par le syst√®me de surveillance. üõ°Ô∏è
                </div>
            </div>
        </body>
        </html>
        """

        return report

    def send_email(self, subject, body):
        """
        Envoie un email en utilisant l'API SendGrid.

        Cette fonction r√©alise les √©tapes suivantes :
        1. **Initialisation de l'API SendGrid** :
        - Utilise la cl√© API de SendGrid (`self.sendgrid_api_key`) pour configurer l'acc√®s √† l'API.
        2. **Pr√©paration du contenu de l'email** :
        - D√©finit l'exp√©diteur (`from_email`), le destinataire (`to_email`), le sujet (`subject`) 
            et le corps de l'email (`body`), qui est en format HTML.
        3. **Envoi de l'email** :
        - Envoie l'email via l'API SendGrid en utilisant la m√©thode `send.post`.
        4. **Gestion des erreurs** :
        - Si l'envoi √©choue, un message d'erreur est affich√©.

        Args:
            subject (str): Le sujet de l'email.
            body (str): Le contenu de l'email en format HTML.

        Returns:
            None: Si l'email est envoy√© avec succ√®s, aucun retour n'est g√©n√©r√©, 
                sinon un message d'erreur est imprim√©.
        """
        
        # Initialisation de l'API SendGrid
        sg = sendgrid.SendGridAPIClient(api_key=self.sendgrid_api_key)
        
        # Cr√©ation des objets pour l'exp√©diteur, le destinataire et le contenu
        from_email = Email(self.from_email)
        to_email = To(self.recipient_email)
        content = Content("text/html", body)
        
        # Cr√©ation de l'objet Mail avec les informations n√©cessaires
        mail = Mail(from_email, to_email, subject, content)

        try:
            # Envoi de l'email via l'API SendGrid
            response = sg.client.mail.send.post(request_body=mail.get())
            print(f"Email envoy√© avec succ√®s: {response.status_code}")
        except Exception as e:
            # Si une erreur survient, affichage du message d'erreur
            print(f"Erreur, email non-envoy√©: {e}")

    def run_report(self):
        """
        Ex√©cute le rapport journalier de s√©curit√© et l'envoie par email.

        Cette fonction r√©alise les √©tapes suivantes :
        1. **R√©cup√©ration des logs journaliers** :
        - Utilise la m√©thode `query_daily_logs()` pour obtenir les logs du jour √† analyser.
        2. **G√©n√©ration du rapport** :
        - Utilise la m√©thode `generate_report()` pour cr√©er un rapport HTML contenant les statistiques et autres informations pertinentes sur les logs.
        3. **Envoi du rapport par email** :
        - Utilise la m√©thode `send_email()` pour envoyer l'email avec le rapport g√©n√©r√© en pi√®ce jointe dans le corps du message.

        Returns:
            None: Cette fonction n'a pas de valeur de retour. Elle ex√©cute des actions (g√©n√©rer et envoyer un rapport).
        """
        
        # R√©cup√©rer les logs de la journ√©e
        logs = self.query_logs()
        
        # G√©n√©rer un rapport √† partir des logs r√©cup√©r√©s
        report = self.generate_report(logs)

        # Envoi du rapport par email
        self.send_email(subject="Rapport de s√©curit√© journalier", body=report)
